{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d064cf75-4983-4c05-ad6a-994e1dad9de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "  code by Tae Hwan Jung(Jeff Jung) @graykode, Derek Miller @dmmiller612, modify by wmathor\n",
    "  Reference : https://github.com/jadore801120/attention-is-all-you-need-pytorch\n",
    "              https://github.com/JayParks/transformer\n",
    "'''\n",
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2431eaa1-16a1-4d01-9422-4d3f68a36531",
   "metadata": {},
   "source": [
    "本代码侧重于理解Transformer的内部结构，所以仅用简单的例句（德语-->英语）做输入数据\n",
    "<br>\n",
    "注意：enumerate函数用于将一个可遍历的数据对象(如列表、元组或字符串)组合为一个索引序列，同时列出数据和数据下标，一般用在for循环当中。\n",
    "    用enumerate时，打印出来先为索引下标，后为下标对应内容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb38936b-bb42-4e4d-a907-11357922ffc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# S: 解码输入开始的符号\n",
    "# E: 解码输出开始的符号\n",
    "# P: 如果当前批处理数据大小小于时间步长，则将填充空白序列的符号\n",
    "sentences = [\n",
    "        # enc_input 编码器输入     dec_input 解码器输入   dec_output 解码器输出\n",
    "        ['ich mochte ein bier P', 'S i want a beer .', 'i want a beer . E'],\n",
    "        ['ich mochte ein cola P', 'S i want a coke .', 'i want a coke . E']\n",
    "]\n",
    "\n",
    "# Padding Should be Zero\n",
    "src_vocab = {'P' : 0, 'ich' : 1, 'mochte' : 2, 'ein' : 3, 'bier' : 4, 'cola' : 5}\n",
    "src_vocab_size = len(src_vocab)    #output: 6\n",
    "\n",
    "tgt_vocab = {'P' : 0, 'i' : 1, 'want' : 2, 'a' : 3, 'beer' : 4, 'coke' : 5, 'S' : 6, 'E' : 7, '.' : 8}\n",
    "idx2word = {i: w for i, w in enumerate(tgt_vocab)}    #output: {0: 'P', 1: 'i',.......,5: 'coke', 6: 'S', 7: 'E', 8: '.'}\n",
    "tgt_vocab_size = len(tgt_vocab)    #output: 9\n",
    "\n",
    "src_len = 5 # enc_input max sequence length\n",
    "tgt_len = 6 # dec_input(=dec_output) max sequence length\n",
    "\n",
    "# Transformer Parameters\n",
    "d_model = 512  # Embedding Size\n",
    "d_ff = 2048 # FeedForward dimension 也可以理解为隐藏神经元的个数\n",
    "d_k = d_v = 64  # dimension of K(=Q), V\n",
    "n_layers = 6  # number of Encoder and Decoder Layer\n",
    "n_heads = 8  # number of heads in Multi-Head Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57897aa6-2c16-45e3-8e0d-96ebb6a13e8c",
   "metadata": {},
   "source": [
    "数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96934e20-6110-4bc9-acb6-d3a2c6e7cb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data(sentences):\n",
    "    enc_inputs, dec_inputs, dec_outputs = [], [], []\n",
    "    for i in range(len(sentences)):\n",
    "      enc_input = [[src_vocab[n] for n in sentences[i][0].split()]] # [[1, 2, 3, 4, 0], [1, 2, 3, 5, 0]]\n",
    "      dec_input = [[tgt_vocab[n] for n in sentences[i][1].split()]] # [[6, 1, 2, 3, 4, 8], [6, 1, 2, 3, 5, 8]]\n",
    "      dec_output = [[tgt_vocab[n] for n in sentences[i][2].split()]] # [[1, 2, 3, 4, 8, 7], [1, 2, 3, 5, 8, 7]]\n",
    "\n",
    "      enc_inputs.extend(enc_input)\n",
    "      dec_inputs.extend(dec_input)\n",
    "      dec_outputs.extend(dec_output)\n",
    "\n",
    "    return torch.LongTensor(enc_inputs), torch.LongTensor(dec_inputs), torch.LongTensor(dec_outputs)\n",
    "\n",
    "enc_inputs, dec_inputs, dec_outputs = make_data(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77f82ef-9cde-4ab8-9d13-f3541bdf5ad1",
   "metadata": {},
   "source": [
    "制作数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9f211eb-d748-423a-bb5a-8002b4aed04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataSet(Data.Dataset):\n",
    "  def __init__(self, enc_inputs, dec_inputs, dec_outputs):\n",
    "    super(MyDataSet, self).__init__()\n",
    "    self.enc_inputs = enc_inputs\n",
    "    self.dec_inputs = dec_inputs\n",
    "    self.dec_outputs = dec_outputs\n",
    "  \n",
    "  def __len__(self):\n",
    "    return self.enc_inputs.shape[0]\n",
    "  \n",
    "  def __getitem__(self, idx):\n",
    "    return self.enc_inputs[idx], self.dec_inputs[idx], self.dec_outputs[idx]\n",
    "\n",
    "loader = Data.DataLoader(MyDataSet(enc_inputs, dec_inputs, dec_outputs), batch_size = 2, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fdf08e-f02b-4f6a-b1f9-cb3778011923",
   "metadata": {},
   "source": [
    "Positional Encoding 初始公式：<br>\n",
    "$PE\\left( {pos,2i} \\right) = \\sin (pos/{10000^{2i/{d_{model}}}})$\n",
    "<br>\n",
    "$PE\\left( {pos,2i+1} \\right) = \\cos (pos/{10000^{2i/{d_{model}}}})$\n",
    "<br>\n",
    "<br>\n",
    "<b>解读div_term</b><br>\n",
    "下面代码中div_term的表达式如下，其中torch.arange(0, d_model, 2)可以理解为2i\n",
    "$divterm = {e^{2i*(\\frac{{ - \\log 10000}}{{{d_{model}}}})}} = {e^{ - \\log 10000*\\frac{{2i}}{{{d_{model}}}}}} = {({e^{\\log 10000}})^{ - \\frac{{2i}}{{{d_{model}}}}}} = {10000^{ - \\frac{{2i}}{{{d_{model}}}}}} = {({10000^{\\frac{{2i}}{{{d_{model}}}}}})^{ - 1}} = \\frac{1}{{{{10000}^{\\frac{{2i}}{{{d_{model}}}}}}}}$\n",
    "<br>\n",
    "<br>\n",
    "<b>解读pe[:, 0::2]</b><br>\n",
    "\":\"可以理解为将整个pe进行切片操作，若是\":1\"则将pe的第一层切片，\":2\"则将pe的前两层切片<br>\n",
    "\"0::2\"表示从第0个位置开始，每隔2个取数，如果把0省略（::2）效果一样；同理\"1::2\"表示从第1个位置开始，每隔2个取数；“::-1”中-1表示逆序读取\n",
    "<br>\n",
    "<br>\n",
    "<b>解读register_buffer()</b><br>\n",
    "通过register_buffer()登记过的张量：会自动成为模型中的参数，随着模型移动（gpu/cpu）而移动，但是不会随着梯度进行更新。buffer与parameter具有 “同等地位”，所以将某些不需要更新的变量“拿出来”作为buffer，可能更方便操作，可读性也更高，对Paramter的各种操作（固定网络的等）可能也不会“误伤到” buffer这种变量。buffer最重要的意义应该是需要得到梯度信息时，不会更新因为optimizer而更新，这也是parameter所不能代替的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7bab9bca-2e27-4fd3-83eb-e16195dce508",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)  #output: max_len*d_model的全0的tensor矩阵\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1) #output: max_len*1的tensor矩阵--->tensor([[0.],[1.],[2.],...,[max_len-1.],])\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1) #先升维再将0维和1维转置\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        x: [seq_len, batch_size, d_model]\n",
    "        '''\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046ab58c-40fa-46d1-82a3-4c161e7057c9",
   "metadata": {},
   "source": [
    "由于在Encoder和Decoder中都有注意力机制，因此就无法确定函数中seq_len的值：如果在Encoder中调用，seq_len=src_len；如果在Decoder中调用，seq_len=src_len or tgt_len（Decoder有两次注意力机制）<br>\n",
    "<br>\n",
    "在Self Attention的计算过程中，通常使用 mini-batch 来计算，也就是一次计算多句话，而一个 mini-batch 是由多个不等长的句子组成的，所以需要按照这个 mini-batch 中最大的句长对剩余的句子进行补齐，一般用 0 进行填充，这个过程叫做 padding\n",
    "<br>\n",
    "<br>\n",
    "<b>解读get_attn_pad_mask</b><br>\n",
    "这里的mask可以理解为\"用作与补齐句子的掩码（用0表示）\"<br>\n",
    "1.在Encoder中，当输入数据与最大句长不相符时，在输入数据后面补0（将序列中P字符标记为0）<br>\n",
    "2.扩展维度 [batch_size, len_k] --> [batch_size, 1, len_k] --> [batch_size, len_q, len_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5842f82f-1156-46f6-8d8e-6625284ee5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attn_pad_mask(seq_q, seq_k):\n",
    "    '''\n",
    "    seq_q: [batch_size, seq_len]\n",
    "    seq_k: [batch_size, seq_len]\n",
    "    seq_len could be src_len or it could be tgt_len\n",
    "    seq_len in seq_q and seq_len in seq_k maybe not equal\n",
    "    '''\n",
    "    batch_size, len_q = seq_q.size()\n",
    "    batch_size, len_k = seq_k.size()\n",
    "    # eq(zero) is PAD token\n",
    "    pad_attn_mask = seq_k.data.eq(0).unsqueeze(1)  # 扩展维度，[batch_size, len_k] --> [batch_size, 1, len_k], False is masked\n",
    "    print(\"pad_attn_mask = {}\".format(pad_attn_mask))\n",
    "    print(\"pad_attn_mask.shape = {}\".format(pad_attn_mask.shape))\n",
    "    return pad_attn_mask.expand(batch_size, len_q, len_k)  # [batch_size, len_q, len_k]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00abd385-3340-4f8c-aeaa-fe2108b41b05",
   "metadata": {},
   "source": [
    "<b>解读get_attn_subsequence_mask</b><br>\n",
    "该方法只能在Decoder中用到，主要作用为屏蔽未来时刻的词信息，[batch_size, tgt_len] --> [batch_size, tgt_len, tgt_len]<br>\n",
    "首先通过 np.ones() 生成一个全 1 的方阵，然后通过 np.triu() 生成一个上三角矩阵<br>\n",
    "其中np.triu(data, 0)为生成上三角（对角线上为正常数字，对角线下方为0），np.tril(data, 0)为生成下三角。1和-1表示对角线上移或下移1个位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbe9f30-1e2a-4878-828b-1249b116bff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attn_subsequence_mask(seq):\n",
    "    '''\n",
    "    seq: [batch_size, tgt_len]\n",
    "    '''\n",
    "    attn_shape = [seq.size(0), seq.size(1), seq.size(1)] # [batch_size, tgt_len, tgt_len]\n",
    "    subsequence_mask = np.triu(np.ones(attn_shape), k=1) # Upper triangular matrix, return type = array\n",
    "    subsequence_mask = torch.from_numpy(subsequence_mask).byte() # type: array --> tensor\n",
    "    return subsequence_mask # [batch_size, tgt_len, tgt_len]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b415c6-95e9-4bf1-a872-89429414f359",
   "metadata": {},
   "source": [
    "$Attention(Q,K,V) = softmax(\\frac{{Q{K^T}}}{{\\sqrt {{d_k}} }})V$<br>\n",
    "<br>\n",
    "<b>解读scores</b><br>\n",
    "scores.masked_fill_(attn_mask, -1e9) 将True的地方填充为self自身（把需要屏蔽的地方屏蔽掉，比如说Encoder中为了保持最大句长而填充的0）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46b58c46-ec25-48a9-8dd0-349cdbbfa6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "\n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        '''\n",
    "        Q: [batch_size, n_heads, len_q, d_k] 由于Q和K的维度一致，所以这里统一用d_k表示\n",
    "        K: [batch_size, n_heads, len_k, d_k]\n",
    "        V: [batch_size, n_heads, len_v(=len_k), d_v]\n",
    "        attn_mask: [batch_size, n_heads, seq_len, seq_len]\n",
    "        '''\n",
    "        scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(d_k) # scores : [batch_size, n_heads, len_q, len_k]\n",
    "        scores.masked_fill_(attn_mask, -1e9) # Fills elements of self tensor with value where mask is True.\n",
    "        \n",
    "        attn = nn.Softmax(dim=-1)(scores)\n",
    "        context = torch.matmul(attn, V) # [batch_size, n_heads, len_q, d_v]\n",
    "        return context, attn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed4dc9a-90df-43a2-95ba-e52f0bac6c0b",
   "metadata": {},
   "source": [
    "完整代码中一定会有三处地方调用 MultiHeadAttention()：<br>\n",
    "Encoder Layer 调用一次，传入的 input_Q、input_K、input_V 全部都是 enc_inputs；<br>\n",
    "Decoder Layer 中两次调用，第一次传入的全是 dec_inputs，第二次传入的分别是 dec_outputs，enc_outputs，enc_outputs<br>\n",
    "\n",
    "<b>这段的代码没太看懂，师兄要是看懂了给我讲讲</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4e08587-513f-41bc-a211-e60d99bfa70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.W_Q = nn.Linear(d_model, d_k * n_heads, bias=False)\n",
    "        self.W_K = nn.Linear(d_model, d_k * n_heads, bias=False)\n",
    "        self.W_V = nn.Linear(d_model, d_v * n_heads, bias=False)\n",
    "        self.fc = nn.Linear(n_heads * d_v, d_model, bias=False)\n",
    "    def forward(self, input_Q, input_K, input_V, attn_mask):\n",
    "        '''\n",
    "        input_Q: [batch_size, len_q, d_model] d_model就是词向量的维度embedding_dimension\n",
    "        input_K: [batch_size, len_k, d_model]\n",
    "        input_V: [batch_size, len_v(=len_k), d_model]\n",
    "        attn_mask: [batch_size, seq_len, seq_len]\n",
    "        '''\n",
    "        residual, batch_size = input_Q, input_Q.size(0)\n",
    "        # (B, S, D) -proj-> (B, S, D_new) -split-> (B, S, H, W) -trans-> (B, H, S, W)\n",
    "        Q = self.W_Q(input_Q).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # Q: [batch_size, n_heads, len_q, d_k]\n",
    "        K = self.W_K(input_K).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # K: [batch_size, n_heads, len_k, d_k]\n",
    "        V = self.W_V(input_V).view(batch_size, -1, n_heads, d_v).transpose(1,2)  # V: [batch_size, n_heads, len_v(=len_k), d_v]\n",
    "\n",
    "        attn_mask = attn_mask.unsqueeze(1).repeat(1, n_heads, 1, 1) # attn_mask : [batch_size, n_heads, seq_len, seq_len]\n",
    "\n",
    "        # context: [batch_size, n_heads, len_q, d_v], attn: [batch_size, n_heads, len_q, len_k]\n",
    "        context, attn = ScaledDotProductAttention()(Q, K, V, attn_mask)\n",
    "        context = context.transpose(1, 2).reshape(batch_size, -1, n_heads * d_v) # context: [batch_size, len_q, n_heads * d_v]\n",
    "        output = self.fc(context) # [batch_size, len_q, d_model]\n",
    "        return nn.LayerNorm(d_model).cuda()(output + residual), attn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75282802-d7c3-4364-bee5-419a4636edc3",
   "metadata": {},
   "source": [
    "前馈神经网络的公式为<br>\n",
    "残差连接：${X_{{\\rm{attention}}}} = X + {X_{{\\rm{attention}}}}$<br>\n",
    "LayerNorm：${X_{{\\rm{attention}}}} = LayerNorm\\left( {{X_{{\\rm{attention}}}}} \\right)$<br>\n",
    "前馈神经网络：${X_{{\\rm{hidden}}}} = L{\\rm{in}}ear\\left( {{\\mathop{\\rm Re}\\nolimits} LU\\left( {Linear\\left( {{X_{{\\rm{attention}}}}} \\right)} \\right)} \\right)$<br>\n",
    "残差连接：${X_{{\\rm{hidden}}}} = {X_{{\\rm{attention}}}} + {X_{{\\rm{hidden}}}}$<br>\n",
    "LayerNorm：${X_{{\\rm{hidden}}}} = LayerNorm\\left( {{X_{{\\rm{hidden}}}}} \\right)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8eb37a21-a8ba-4b71-865b-17e9c8b2e0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoswiseFeedForwardNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_ff, d_model, bias=False)\n",
    "        )\n",
    "    def forward(self, inputs):\n",
    "        '''\n",
    "        inputs: [batch_size, seq_len, d_model]\n",
    "        '''\n",
    "        residual = inputs\n",
    "        output = self.fc(inputs)\n",
    "        return nn.LayerNorm(d_model).cuda()(output + residual) # [batch_size, seq_len, d_model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "472345f1-8d57-4557-90a2-55e8af16129f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.enc_self_attn = MultiHeadAttention()\n",
    "        self.pos_ffn = PoswiseFeedForwardNet()\n",
    "\n",
    "    def forward(self, enc_inputs, enc_self_attn_mask):\n",
    "        '''\n",
    "        enc_inputs: [batch_size, src_len, d_model]\n",
    "        enc_self_attn_mask: [batch_size, src_len, src_len]\n",
    "        '''\n",
    "        # enc_outputs: [batch_size, src_len, d_model], attn: [batch_size, n_heads, src_len, src_len]\n",
    "        # 多头注意力机制\n",
    "        enc_outputs, attn = self.enc_self_attn(enc_inputs, enc_inputs, enc_inputs, enc_self_attn_mask) # enc_inputs to same Q,K,V\n",
    "        # 前馈神经网络\n",
    "        enc_outputs = self.pos_ffn(enc_outputs) # enc_outputs: [batch_size, src_len, d_model]\n",
    "        return enc_outputs, attn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055ab5cb-988b-42fc-b992-1bb9b50a30bd",
   "metadata": {},
   "source": [
    "在 Decoder Layer 中会调用两次 MultiHeadAttention，第一次是计算 Decoder Input 的 self-attention，得到输出 dec_outputs。然后将 dec_outputs 作为生成 Q 的元素，enc_outputs 作为生成 K 和 V 的元素，再调用一次 MultiHeadAttention，得到的是 Encoder 和 Decoder Layer 之间的 context vector。最后将 dec_outptus 做一次维度变换，然后返回"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e5a4002-15af-44cf-a67d-8de80689d86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.dec_self_attn = MultiHeadAttention()\n",
    "        self.dec_enc_attn = MultiHeadAttention()\n",
    "        self.pos_ffn = PoswiseFeedForwardNet()\n",
    "\n",
    "    def forward(self, dec_inputs, enc_outputs, dec_self_attn_mask, dec_enc_attn_mask):\n",
    "        '''\n",
    "        dec_inputs: [batch_size, tgt_len, d_model]\n",
    "        enc_outputs: [batch_size, src_len, d_model]\n",
    "        dec_self_attn_mask: [batch_size, tgt_len, tgt_len]\n",
    "        dec_enc_attn_mask: [batch_size, tgt_len, src_len]\n",
    "        '''\n",
    "        # dec_outputs: [batch_size, tgt_len, d_model], dec_self_attn: [batch_size, n_heads, tgt_len, tgt_len]\n",
    "        dec_outputs, dec_self_attn = self.dec_self_attn(dec_inputs, dec_inputs, dec_inputs, dec_self_attn_mask)\n",
    "        # dec_outputs: [batch_size, tgt_len, d_model], dec_enc_attn: [batch_size, h_heads, tgt_len, src_len]\n",
    "        dec_outputs, dec_enc_attn = self.dec_enc_attn(dec_outputs, enc_outputs, enc_outputs, dec_enc_attn_mask)\n",
    "        dec_outputs = self.pos_ffn(dec_outputs) # [batch_size, tgt_len, d_model]\n",
    "        return dec_outputs, dec_self_attn, dec_enc_attn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe7a717-b491-4322-ab31-3abf00692640",
   "metadata": {},
   "source": [
    "使用 nn.ModuleList() 里面的参数是列表，列表里面存了 n_layers 个 Encoder Layer<br>\n",
    "由于已经控制好了 Encoder Layer 的输入和输出维度相同，所以可以直接用个 for 循环以嵌套的方式，将上一次 Encoder Layer 的输出作为下一次 Encoder Layer 的输入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c93419e-73d2-4d6e-89f3-84366f09bdf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.src_emb = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.pos_emb = PositionalEncoding(d_model)\n",
    "        self.layers = nn.ModuleList([EncoderLayer() for _ in range(n_layers)])\n",
    "\n",
    "    def forward(self, enc_inputs):\n",
    "        '''\n",
    "        enc_inputs: [batch_size, src_len]\n",
    "        '''\n",
    "        #词嵌入\n",
    "        enc_outputs = self.src_emb(enc_inputs) # [batch_size, src_len, d_model]\n",
    "        #位置嵌入\n",
    "        enc_outputs = self.pos_emb(enc_outputs.transpose(0, 1)).transpose(0, 1) # [batch_size, src_len, d_model]\n",
    "        #按最大句长填充每个句子，并mask掉填充部分\n",
    "        enc_self_attn_mask = get_attn_pad_mask(enc_inputs, enc_inputs) # [batch_size, src_len, src_len]\n",
    "        enc_self_attns = []\n",
    "        #重复N个EncoderLayer模块\n",
    "        for layer in self.layers:\n",
    "            #将位置信息和词向量输入EncoderLayer\n",
    "            # enc_outputs: [batch_size, src_len, d_model], enc_self_attn: [batch_size, n_heads, src_len, src_len]\n",
    "            enc_outputs, enc_self_attn = layer(enc_outputs, enc_self_attn_mask)\n",
    "            enc_self_attns.append(enc_self_attn)\n",
    "        return enc_outputs, enc_self_attns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679de13a-da8c-4259-9859-58e33870c6a6",
   "metadata": {},
   "source": [
    "Decoder 中不仅要把 \"pad\"mask 掉，还要 mask 未来时刻的信息，因此就有了下面这三行代码，其中 torch.gt(a, value) 的意思是，将 a 中各个位置上的元素和 value 比较，若大于 value，则该位置取 1，否则取 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d2f57fd4-8e78-4d50-8f82-5308093ec2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.tgt_emb = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.pos_emb = PositionalEncoding(d_model)\n",
    "        self.layers = nn.ModuleList([DecoderLayer() for _ in range(n_layers)])\n",
    "\n",
    "    def forward(self, dec_inputs, enc_inputs, enc_outputs):\n",
    "        '''\n",
    "        dec_inputs: [batch_size, tgt_len]\n",
    "        enc_intpus: [batch_size, src_len]\n",
    "        enc_outputs: [batsh_size, src_len, d_model]\n",
    "        '''\n",
    "        #词嵌入\n",
    "        dec_outputs = self.tgt_emb(dec_inputs) # [batch_size, tgt_len, d_model]\n",
    "        #位置嵌入\n",
    "        dec_outputs = self.pos_emb(dec_outputs.transpose(0, 1)).transpose(0, 1).cuda() # [batch_size, tgt_len, d_model]\n",
    "        #按最大句长填充每个句子，并mask掉填充部分\n",
    "        dec_self_attn_pad_mask = get_attn_pad_mask(dec_inputs, dec_inputs).cuda() # [batch_size, tgt_len, tgt_len]\n",
    "        #将未来时刻的信息mask掉\n",
    "        dec_self_attn_subsequence_mask = get_attn_subsequence_mask(dec_inputs).cuda() # [batch_size, tgt_len, tgt_len]\n",
    "        \n",
    "        #为执行Masked Multi-Head Attention做准备\n",
    "        #将 mask后数据 中各个位置上的元素和 0 比较，若大于 0，则该位置取 1，否则取 0\n",
    "        dec_self_attn_mask = torch.gt((dec_self_attn_pad_mask + dec_self_attn_subsequence_mask), 0).cuda() # [batch_size, tgt_len, tgt_len]\n",
    "\n",
    "        #为执行Multi-Head Attention做准备\n",
    "        dec_enc_attn_mask = get_attn_pad_mask(dec_inputs, enc_inputs) # [batc_size, tgt_len, src_len]\n",
    "\n",
    "        dec_self_attns, dec_enc_attns = [], []\n",
    "        #重复N个DecoderLayer模块\n",
    "        for layer in self.layers:\n",
    "            # dec_outputs: [batch_size, tgt_len, d_model], dec_self_attn: [batch_size, n_heads, tgt_len, tgt_len], dec_enc_attn: [batch_size, h_heads, tgt_len, src_len]\n",
    "            dec_outputs, dec_self_attn, dec_enc_attn = layer(dec_outputs, enc_outputs, dec_self_attn_mask, dec_enc_attn_mask)\n",
    "            dec_self_attns.append(dec_self_attn)\n",
    "            dec_enc_attns.append(dec_enc_attn)\n",
    "        return dec_outputs, dec_self_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3771af-e4e6-4422-9015-3458732a24c4",
   "metadata": {},
   "source": [
    "Transformer 主要就是调用 Encoder 和 Decoder。最后返回 dec_logits 的维度是 [batch_size * tgt_len, tgt_vocab_size]，可以理解为，一个句子，这个句子有 batch_size*tgt_len 个单词，每个单词有 tgt_vocab_size 种情况，取概率最大者"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8bd98ed-fb86-404f-aec3-f70f984fca6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder = Encoder().cuda()\n",
    "        self.decoder = Decoder().cuda()\n",
    "        self.projection = nn.Linear(d_model, tgt_vocab_size, bias=False).cuda()\n",
    "    def forward(self, enc_inputs, dec_inputs):\n",
    "        '''\n",
    "        enc_inputs: [batch_size, src_len]\n",
    "        dec_inputs: [batch_size, tgt_len]\n",
    "        '''\n",
    "        # tensor to store decoder outputs\n",
    "        # outputs = torch.zeros(batch_size, tgt_len, tgt_vocab_size).to(self.device)\n",
    "        \n",
    "        # enc_outputs: [batch_size, src_len, d_model], enc_self_attns: [n_layers, batch_size, n_heads, src_len, src_len]\n",
    "        enc_outputs, enc_self_attns = self.encoder(enc_inputs)\n",
    "        # dec_outpus: [batch_size, tgt_len, d_model], dec_self_attns: [n_layers, batch_size, n_heads, tgt_len, tgt_len], dec_enc_attn: [n_layers, batch_size, tgt_len, src_len]\n",
    "        dec_outputs, dec_self_attns, dec_enc_attns = self.decoder(dec_inputs, enc_inputs, enc_outputs)\n",
    "        dec_logits = self.projection(dec_outputs) # dec_logits: [batch_size, tgt_len, tgt_vocab_size]\n",
    "        return dec_logits.view(-1, dec_logits.size(-1)), enc_self_attns, dec_self_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5edeffdf-35c5-47b1-b816-b4ebd9a9e250",
   "metadata": {},
   "source": [
    "这里的损失函数里面设置了一个参数 ignore_index=0，因为 \"pad\" 这个单词的索引为 0，这样设置以后，就不会计算 \"pad\" 的损失（因为本来 \"pad\" 也没有意义，不需要计算）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "df4bf20f-23f1-47aa-af89-06157fe1df92",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [18]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[0;32m      2\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss(ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m      3\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mSGD(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m, momentum\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.99\u001b[39m)\n",
      "Input \u001b[1;32mIn [14]\u001b[0m, in \u001b[0;36mTransformer.__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28msuper\u001b[39m(Transformer, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m----> 4\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder \u001b[38;5;241m=\u001b[39m \u001b[43mEncoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder \u001b[38;5;241m=\u001b[39m Decoder()\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprojection \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(d_model, tgt_vocab_size, bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mcuda()\n",
      "File \u001b[1;32mD:\\soft\\Miniconda\\envs\\pytorch_practice_py38\\lib\\site-packages\\torch\\nn\\modules\\module.py:680\u001b[0m, in \u001b[0;36mModule.cuda\u001b[1;34m(self, device)\u001b[0m\n\u001b[0;32m    663\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcuda\u001b[39m(\u001b[38;5;28mself\u001b[39m: T, device: Optional[Union[\u001b[38;5;28mint\u001b[39m, device]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[0;32m    664\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Moves all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[0;32m    665\u001b[0m \n\u001b[0;32m    666\u001b[0m \u001b[38;5;124;03m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    678\u001b[0m \u001b[38;5;124;03m        Module: self\u001b[39;00m\n\u001b[0;32m    679\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 680\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\soft\\Miniconda\\envs\\pytorch_practice_py38\\lib\\site-packages\\torch\\nn\\modules\\module.py:570\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    568\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[0;32m    569\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 570\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    573\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    574\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    575\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    580\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    581\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mD:\\soft\\Miniconda\\envs\\pytorch_practice_py38\\lib\\site-packages\\torch\\nn\\modules\\module.py:593\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    589\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    590\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    591\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    592\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 593\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    594\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    595\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[1;32mD:\\soft\\Miniconda\\envs\\pytorch_practice_py38\\lib\\site-packages\\torch\\nn\\modules\\module.py:680\u001b[0m, in \u001b[0;36mModule.cuda.<locals>.<lambda>\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    663\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcuda\u001b[39m(\u001b[38;5;28mself\u001b[39m: T, device: Optional[Union[\u001b[38;5;28mint\u001b[39m, device]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[0;32m    664\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Moves all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[0;32m    665\u001b[0m \n\u001b[0;32m    666\u001b[0m \u001b[38;5;124;03m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    678\u001b[0m \u001b[38;5;124;03m        Module: self\u001b[39;00m\n\u001b[0;32m    679\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 680\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply(\u001b[38;5;28;01mlambda\u001b[39;00m t: \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mD:\\soft\\Miniconda\\envs\\pytorch_practice_py38\\lib\\site-packages\\torch\\cuda\\__init__.py:208\u001b[0m, in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    205\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    206\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m--> 208\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    209\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[0;32m    211\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "model = Transformer().cuda()\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-3, momentum=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e552c105-c356-4ee8-a215-993b7b468f93",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m enc_inputs, dec_inputs, dec_outputs \u001b[38;5;129;01min\u001b[39;00m loader:\n\u001b[0;32m      3\u001b[0m   \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m  enc_inputs: [batch_size, src_len]\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;03m  dec_inputs: [batch_size, tgt_len]\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;124;03m  dec_outputs: [batch_size, tgt_len]\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124;03m  '''\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m   enc_inputs, dec_inputs, dec_outputs \u001b[38;5;241m=\u001b[39m \u001b[43menc_inputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, dec_inputs\u001b[38;5;241m.\u001b[39mcuda(), dec_outputs\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[0;32m      9\u001b[0m   \u001b[38;5;66;03m# outputs: [batch_size * tgt_len, tgt_vocab_size]\u001b[39;00m\n\u001b[0;32m     10\u001b[0m   outputs, enc_self_attns, dec_self_attns, dec_enc_attns \u001b[38;5;241m=\u001b[39m model(enc_inputs, dec_inputs)\n",
      "File \u001b[1;32mD:\\soft\\Miniconda\\envs\\pytorch_practice_py38\\lib\\site-packages\\torch\\cuda\\__init__.py:208\u001b[0m, in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    205\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    206\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m--> 208\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    209\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[0;32m    211\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "for epoch in range(1000):\n",
    "    for enc_inputs, dec_inputs, dec_outputs in loader:\n",
    "      '''\n",
    "      enc_inputs: [batch_size, src_len]\n",
    "      dec_inputs: [batch_size, tgt_len]\n",
    "      dec_outputs: [batch_size, tgt_len]\n",
    "      '''\n",
    "      enc_inputs, dec_inputs, dec_outputs = enc_inputs.cuda(), dec_inputs.cuda(), dec_outputs.cuda()\n",
    "      # outputs: [batch_size * tgt_len, tgt_vocab_size]\n",
    "      outputs, enc_self_attns, dec_self_attns, dec_enc_attns = model(enc_inputs, dec_inputs)\n",
    "      loss = criterion(outputs, dec_outputs.view(-1))\n",
    "      print('Epoch:', '%04d' % (epoch + 1), 'loss =', '{:.6f}'.format(loss))\n",
    "\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b0fea4-e0ba-4349-90bc-30d79ed3c2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decoder(model, enc_input, start_symbol):\n",
    "    \"\"\"\n",
    "    For simplicity, a Greedy Decoder is Beam search when K=1. This is necessary for inference as we don't know the\n",
    "    target sequence input. Therefore we try to generate the target input word by word, then feed it into the transformer.\n",
    "    Starting Reference: http://nlp.seas.harvard.edu/2018/04/03/attention.html#greedy-decoding\n",
    "    :param model: Transformer Model\n",
    "    :param enc_input: The encoder input\n",
    "    :param start_symbol: The start symbol. In this example it is 'S' which corresponds to index 4\n",
    "    :return: The target input\n",
    "    \"\"\"\n",
    "    enc_outputs, enc_self_attns = model.encoder(enc_input)\n",
    "    dec_input = torch.zeros(1, 0).type_as(enc_input.data)\n",
    "    terminal = False\n",
    "    next_symbol = start_symbol\n",
    "    while not terminal:         \n",
    "        dec_input = torch.cat([dec_input.detach(),torch.tensor([[next_symbol]],dtype=enc_input.dtype).cuda()],-1)\n",
    "        dec_outputs, _, _ = model.decoder(dec_input, enc_input, enc_outputs)\n",
    "        projected = model.projection(dec_outputs)\n",
    "        prob = projected.squeeze(0).max(dim=-1, keepdim=False)[1]\n",
    "        next_word = prob.data[-1]\n",
    "        next_symbol = next_word\n",
    "        if next_symbol == tgt_vocab[\".\"]:\n",
    "            terminal = True\n",
    "        print(next_word)            \n",
    "    return dec_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51361b4d-f1a6-4879-936b-f14b0ca6502e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "enc_inputs, _, _ = next(iter(loader))\n",
    "enc_inputs = enc_inputs.cuda()\n",
    "for i in range(len(enc_inputs)):\n",
    "    greedy_dec_input = greedy_decoder(model, enc_inputs[i].view(1, -1), start_symbol=tgt_vocab[\"S\"])\n",
    "    predict, _, _, _ = model(enc_inputs[i].view(1, -1), greedy_dec_input)\n",
    "    predict = predict.data.max(1, keepdim=True)[1]\n",
    "    print(enc_inputs[i], '->', [idx2word[n.item()] for n in predict.squeeze()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04231a9f-df8e-4f5c-b7b2-de8883446c8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
